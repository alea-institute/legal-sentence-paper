\section{Discussion and Conclusion}
We introduced \textit{NUPunkt} and \textit{CharBoundary}, two sentence boundary detection libraries designed for legal text processing at scale. Our comprehensive evaluation demonstrates that these specialized approaches significantly outperform general-purpose methods in precision-critical legal contexts. As shown in Table~\ref{tab:sbd-precision-performance}, \textit{NUPunkt} and \textit{CharBoundary} reduce false positive boundaries by 40-60\% compared to baseline methods.

\subsection{Implications for Legal NLP and RAG Systems}
The performance improvements showcased by our approaches carry profound implications for both legal natural language processing (NLP) and modern retrieval-augmented generation systems.   By enhancing sentence boundary precision from 70\% to 90\%, our methods reduce fragmentation errors by roughly two-thirds, and with precision nearing 99\%—as demonstrated by NUPunkt on the BVA dataset—these errors are almost entirely eliminated, ensuring that the integrity of legal concepts remains intact across text chunks. Additionally, the throughput improvements, ranging from 10x to 100x times faster than transformer-based approaches, make it possible to process vast case law databases or regulatory repositories in mere hours rather than days, significantly accelerating workflows. Furthermore, the reliance on CPU-only implementation with minimal dependencies slashes computational demands compared to transformer-based methods, allowing for scalable deployment without the need for specialized hardware. Together, these advancements pave the way for more accurate, efficient, and accessible legal NLP systems.

\subsection{Application Selection Guide}
Drawing from our evaluation results, we provide some practical guidance for choosing the most suitable method tailored to specific needs.  For precision-critical retrieval-augmented generation (RAG) applications, where incorrect sentence splits could undermine context preservation, NUPunkt stands out with its high precision and ability to process 10+ million characters per second, making it an ideal choice.  When handling legal documents rich with case citations and regulatory references, CharBoundary (large) proves effective, delivering 96.8\% precision on cybercrime documents and 95.4\% on intellectual property texts, ensuring reliable performance across complex legal texts.  In resource-constrained environments where computational power is limited, CharBoundary (small) offers a compelling solution, maintaining over 92\% precision across all legal datasets while keeping resource demands low.  These recommendations enable practitioners to optimize their approach based on precision, processing speed, and available resources.

\subsection{Limitations and Future Work}
While our approaches demonstrate substantial improvements, limitations include language scope (primarily English), legal subdomain coverage, and adaptation to non-standard document formats. Future work includes multi-lingual extensions, integration with end-to-end legal NLP pipelines, and hybrid approaches combining rule-based and ML methods.

We are also developing a Rust implementation of CharBoundary that will transpile the random forest/decision tree models into explicit if-else code statements. This approach, which eliminates runtime model interpretation overhead, is expected to deliver significant performance improvements while maintaining identical accuracy, enabling even faster processing for high-volume applications.

By releasing these implementations as open-source software and providing an interactive demonstration at \url{https://sentences.aleainstitute.ai/}, we contribute practical tools for legal NLP research and applications, addressing a critical gap in text processing capabilities for precision-sensitive legal document analysis at scale.

