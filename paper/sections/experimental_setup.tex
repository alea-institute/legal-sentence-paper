\section{Experimental Setup}
To evaluate \textit{NUPunkt} and \textit{CharBoundary}, we conducted a comprehensive benchmark against established SBD methods across diverse legal datasets, with a focus on precision and throughput as critical metrics for legal applications.

\subsection{Datasets}
We evaluated all approaches on five diverse legal datasets from two collections: the ALEA Legal Benchmark \cite{alea-benchmark} and the MultiLegalSBD collection \cite{multilegalSBD} (SCOTUS, Cyber Crime, BVA, and IP cases). Collectively, these datasets comprise over 25,000 documents and 197,000 annotated sentence boundaries across a range of legal text types with different annotation formats and complexity levels. Detailed dataset statistics are available in Appendix \ref{appendix:Datasets}.

\subsection{Baseline Methods}
We compared our approaches against established baselines:
\begin{itemize}
    \item \textit{NLTK Punkt} \cite{kiss2006unsupervised}: Unsupervised statistical approach (62.1\% precision on legal text)
    \item \textit{spaCy} models \cite{spacy}: \texttt{en\_core\_web\_small} and \texttt{en\_core\_web\_lg} (64.3-64.7\% precision)
    \item \textit{pySBD} \cite{sadvilkar2020pysbd}: Rule-based approach
\end{itemize}

\subsection{Evaluation Methodology}
Performance metrics were calculated at the character level on a standard workstation CPU (Intel Core i7-12700K). We measured precision, recall, F1 score, throughput (characters processed per second), and peak memory usage.

Our results demonstrate that \textit{NUPunkt} achieves 91.1\% precision while processing 10 million characters per second with modest memory requirements (432 MB), providing a 29-32\% precision improvement over standard tools. \textit{CharBoundary} models offer balanced precision-recall tradeoffs, with the large model achieving the highest F1 score (0.782) among all tested methods, with throughput ranging from 518K-748K characters per second depending on model size.

\subsection{Implementation and Availability}
All experiments were conducted on standard CPU hardware without specialized accelerators, reflecting typical deployment environments for legal text processing. Both libraries run efficiently on CPU-only systems, making them suitable for deployment in restricted environments or large-scale production systems. \textit{NUPunkt} is implemented in pure Python with zero external dependencies, while \textit{CharBoundary} relies only on scikit-learn and optional ONNX runtime integration for optimized performance. Both libraries are available under the MIT license, with complete source code and interactive demonstrations at \url{https://sentences.aleainstitute.ai/}.

